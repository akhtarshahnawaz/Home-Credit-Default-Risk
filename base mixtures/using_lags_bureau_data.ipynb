{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"using_lags_bureau_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakhtar0092/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Activation,InputLayer\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Files & Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/application_train.csv', usecols=[\"SK_ID_CURR\", \"TARGET\"])\n",
    "test = pd.read_csv('../data/application_test.csv', usecols=[\"SK_ID_CURR\"])\n",
    "bureau = pd.read_csv(\"../data/bureau.csv\").sort_values(by =[\"SK_ID_CURR\",\"DAYS_CREDIT\"]).reset_index(drop=True)\n",
    "bureau_balance = pd.read_csv(\"../data/bureau_balance.csv\")\n",
    "\n",
    "bureau_balance[\"STATUS\"] = LabelEncoder().fit_transform(bureau_balance[\"STATUS\"].fillna(\"NAN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features from Bureau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE: 307511, TEST SHAPE: 48744, BUREAU SHAPE: 1716428, BUREAU BALANCE SHAPE: 27299925\n"
     ]
    }
   ],
   "source": [
    "# Generating Integer Columns\n",
    "bureau[\"credit_is_active\"] = (bureau[\"CREDIT_ACTIVE\"] == \"Active\").astype(int)\n",
    "bureau[\"credit_is_closed\"] = (bureau[\"CREDIT_ACTIVE\"] == \"Closed\").astype(int)\n",
    "bureau[\"credit_is_sold\"] = (bureau[\"CREDIT_ACTIVE\"] == \"Sold\").astype(int)\n",
    "\n",
    "# Various Ratio Features\n",
    "r2annuity_cols = [\"AMT_CREDIT_SUM\", \"AMT_CREDIT_MAX_OVERDUE\",\"AMT_CREDIT_SUM_DEBT\",\"AMT_CREDIT_SUM_LIMIT\", \"AMT_CREDIT_SUM_OVERDUE\"]\n",
    "for col in r2annuity_cols:\n",
    "    bureau[col+\"_to_annuity_ratio\"] = bureau[col]/bureau[\"AMT_ANNUITY\"].astype(\"float32\")\n",
    "\n",
    "r2creditsum_cols = [\"AMT_CREDIT_SUM_DEBT\",\"AMT_CREDIT_SUM_LIMIT\"]\n",
    "for col in r2creditsum_cols:\n",
    "    bureau[col+\"_to_amt_credit_sum\"] = bureau[col]/bureau[\"AMT_CREDIT_SUM\"].astype(\"float32\")\n",
    "\n",
    "bureau[\"debt_to_limit_ratio\"] = bureau.AMT_CREDIT_SUM_DEBT / bureau.AMT_CREDIT_SUM_LIMIT.astype(\"float32\")\n",
    "bureau[\"overdue_to_debt_ratio\"] = bureau.AMT_CREDIT_SUM_OVERDUE / bureau.AMT_CREDIT_SUM_DEBT.astype(\"float32\")\n",
    "\n",
    "# Generating a Groupby\n",
    "bureau_groupby = bureau.groupby(\"SK_ID_CURR\")\n",
    "\n",
    "# Aggregate Features\n",
    "aggregates_df = pd.DataFrame()\n",
    "aggregates_df[\"total_counts\"] = bureau_groupby[\"CREDIT_ACTIVE\"].count()\n",
    "aggregates_df[\"active_counts\"] = bureau_groupby[\"credit_is_active\"].sum()\n",
    "aggregates_df[\"close_counts\"] = bureau_groupby[\"credit_is_closed\"].sum()\n",
    "aggregates_df[\"sold_counts\"] = bureau_groupby[\"credit_is_sold\"].sum()\n",
    "\n",
    "aggregates_df[\"currency_counts\"] = bureau_groupby[\"CREDIT_CURRENCY\"].nunique()\n",
    "aggregates_df[\"credit_type_counts\"] = bureau_groupby[\"CREDIT_TYPE\"].nunique()\n",
    "aggregates_df[\"min_days_credit\"] = bureau_groupby[\"DAYS_CREDIT\"].min()\n",
    "\n",
    "aggregates_df[\"active_ratio\"] = aggregates_df[\"active_counts\"]/aggregates_df[\"total_counts\"].astype(\"float32\")\n",
    "aggregates_df[\"close_ratio\"] = aggregates_df[\"close_counts\"]/aggregates_df[\"total_counts\"].astype(\"float32\")\n",
    "aggregates_df[\"sold_ratio\"] = aggregates_df[\"sold_counts\"]/aggregates_df[\"total_counts\"].astype(\"float32\")\n",
    "aggregates_df[\"loan_type_diversification\"] = aggregates_df[\"credit_type_counts\"]/aggregates_df[\"total_counts\"].astype(\"float32\")\n",
    "\n",
    "# Various Difference Rate Features\n",
    "p_change_cols =[\"DAYS_CREDIT_ENDDATE\",\"CREDIT_DAY_OVERDUE\"]\n",
    "p_change_cols+=[\"AMT_CREDIT_SUM\", \"AMT_CREDIT_MAX_OVERDUE\",\"AMT_CREDIT_SUM_DEBT\",\"AMT_CREDIT_SUM_LIMIT\",\"AMT_CREDIT_SUM_OVERDUE\"]\n",
    "p_change_cols+=[col+\"_to_annuity_ratio\" for col in r2annuity_cols]\n",
    "p_change_cols+=[col+\"_to_amt_credit_sum\" for col in r2creditsum_cols]\n",
    "p_change_cols+=[\"debt_to_limit_ratio\",\"overdue_to_debt_ratio\"]\n",
    "\n",
    "p_changes = bureau_groupby[p_change_cols].diff().bfill()/bureau_groupby[p_change_cols].shift(1).bfill().astype(\"float32\")\n",
    "p_changes.columns = [i+\"_diff\" for i in p_change_cols]\n",
    "bureau = bureau.merge(p_changes, how=\"left\", left_on = \"SK_ID_CURR\", right_index = True)\n",
    "del p_changes\n",
    "gc.collect()\n",
    "\n",
    "# Other Generated Features\n",
    "bureau[\"DAYS_CREDIT_diff_mean\"] = bureau_groupby[\"DAYS_CREDIT\"].diff().bfill().mean()\n",
    "bureau[\"DAYS_CREDIT_diff_std\"] = bureau_groupby[\"DAYS_CREDIT\"].diff().bfill().std()\n",
    "bureau[\"ratio_days_credit\"] = bureau[\"DAYS_CREDIT\"]/bureau[\"SK_ID_CURR\"].map(aggregates_df[\"min_days_credit\"]).astype(\"float32\")\n",
    "\n",
    "del bureau_groupby\n",
    "gc.collect()\n",
    "\n",
    "for col in [c for c in bureau.columns if bureau[c].dtype == \"object\"]:\n",
    "        bureau[col] = LabelEncoder().fit_transform(bureau[col])\n",
    "print \"TRAIN SHAPE: {}, TEST SHAPE: {}, BUREAU SHAPE: {}, BUREAU BALANCE SHAPE: {}\".format(train.shape[0], test.shape[0], bureau.shape[0], bureau_balance.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from Bureau Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUREAU SHAPE: (1716428, 57)\n",
      "BUREAU SHAPE: (1716428, 153)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_means = pd.get_dummies(bureau_balance[[\"SK_ID_BUREAU\",\"STATUS\"]], columns= [\"STATUS\"], dummy_na= True)\n",
    "bb_means = bb_means.groupby(\"SK_ID_BUREAU\")[bb_means.columns.drop([\"SK_ID_BUREAU\"])].apply(lambda x: np.mean(x, axis=0))\n",
    "bureau = bureau.merge(bb_means, how = \"left\", left_on = \"SK_ID_BUREAU\", right_index = True)\n",
    "print \"BUREAU SHAPE: {}\".format(bureau.shape)\n",
    "del bb_means\n",
    "gc.collect()\n",
    "\n",
    "bureau_balance  = bureau_balance.pivot(index = \"SK_ID_BUREAU\",columns = \"MONTHS_BALANCE\", values = \"STATUS\")\n",
    "bureau_balance.columns = [\"BB_PIVOT_{}\".format(i) for i in bureau_balance.columns]\n",
    "bureau = bureau.merge(bureau_balance, how = \"left\", left_on = \"SK_ID_BUREAU\", right_index=True).drop([\"SK_ID_BUREAU\"], axis=1)\n",
    "print \"BUREAU SHAPE: {}\".format(bureau.shape)\n",
    "del bureau_balance\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining all features to make dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SHAPE: (305811, 3661)\n"
     ]
    }
   ],
   "source": [
    "# Aggregating Bureau Dataset by SK_ID_CURR\n",
    "bureau_feats = bureau.columns.drop([\"SK_ID_CURR\"])\n",
    "bureau_groups = bureau.groupby(\"SK_ID_CURR\")\n",
    "group_keys = bureau_groups.groups.keys()\n",
    "\n",
    "data = pd.DataFrame(bureau_groups[bureau_feats].apply(lambda x: list(x[-24:].values.ravel())).values.tolist())\n",
    "data.columns = sum([[\"{}_{}\".format(i,c) for c in bureau_feats] for i in range(int(data.shape[1]/len(bureau_feats)))],[])\n",
    "data[\"SK_ID_CURR\"] = group_keys\n",
    "del bureau\n",
    "gc.collect()\n",
    "\n",
    "# Adding Aggregated Features to Data\n",
    "data = data.merge(aggregates_df, how=\"left\",left_on = \"SK_ID_CURR\", right_index = True)\n",
    "del aggregates_df\n",
    "gc.collect()\n",
    "\n",
    "# Merging to Train\n",
    "data = data.merge(train, how = \"left\", on = \"SK_ID_CURR\")\n",
    "\n",
    "print \"DATA SHAPE: {}\".format(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARING FOR TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_B SHAPE: 263491, TEST_B SHAPE: 42320\n"
     ]
    }
   ],
   "source": [
    "train_b = data.loc[data.TARGET.isin([0,1])].reset_index(drop=True)\n",
    "test_b = data.loc[~data.TARGET.isin([0,1])].reset_index(drop=True)\n",
    "del data\n",
    "gc.collect()\n",
    "\n",
    "target = train_b[\"TARGET\"]\n",
    "train_b_index = train_b[[\"SK_ID_CURR\"]]\n",
    "test_b_index = test_b[[\"SK_ID_CURR\"]]\n",
    "\n",
    "train_b.drop([\"SK_ID_CURR\", \"TARGET\"], axis = 1, inplace = True)\n",
    "test_b.drop([\"SK_ID_CURR\",\"TARGET\"], axis = 1, inplace = True)\n",
    "\n",
    "train_b = train_b\n",
    "test_b = test_b\n",
    "print \"TRAIN_B SHAPE: {}, TEST_B SHAPE: {}\".format(train_b.shape[0], test_b.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "def model_tree(x_train, x_test, y_train, y_test, test, meta_train, meta_test,train_index, test_index,fold_id):\n",
    "    dtrain = lgb.Dataset(x_train, label=y_train)\n",
    "    dval = lgb.Dataset(x_test, label=y_test)\n",
    "    params = {\n",
    "        \"nthread\":14,\n",
    "        \"metric\":\"auc\",\n",
    "        \"objective\": \"binary\",\n",
    "        \"n_estimators\":10000,\n",
    "        \"learning_rate\":0.02,\n",
    "        \"num_leaves\":60,\n",
    "        \"colsample_bytree\":0.9497036,\n",
    "        \"subsample\":0.8715623,\n",
    "        \"max_depth\":8,\n",
    "        \"reg_alpha\":0.04,\n",
    "        \"reg_lambda\":0.073,\n",
    "        \"min_split_gain\":0.0222415,\n",
    "        \"min_child_weight\":40,\n",
    "        \"silent\":-1,\n",
    "        \"verbose\":-1,\n",
    "        \"bagging_seed\" : 42,\n",
    "        \"seed\":98\n",
    "    }\n",
    "    model = lgb.train(params, dtrain, num_boost_round=5000,valid_sets=[dtrain, dval], early_stopping_rounds=200, verbose_eval=100)\n",
    "    meta_train[test_index] = model.predict(x_test, num_iteration=model.best_iteration or 5000)\n",
    "    meta_test.append(model.predict(test, num_iteration=model.best_iteration or 5000))\n",
    "\n",
    "    # Calculate Feature Importance\n",
    "    global feature_importance\n",
    "    gain = model.feature_importance('gain')\n",
    "    fold_feature_importance = pd.DataFrame({'feature':model.feature_name(), 'split':model.feature_importance('split'), 'gain':100 * gain / gain.sum()})\n",
    "    feature_importance = feature_importance.append(fold_feature_importance, ignore_index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakhtar0092/anaconda2/lib/python2.7/site-packages/lightgbm/engine.py:99: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/sakhtar0092/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:657: UserWarning: silent keyword has been found in `params` and will be ignored. Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.683764\tvalid_1's auc: 0.652722\n",
      "[200]\ttraining's auc: 0.71128\tvalid_1's auc: 0.667089\n",
      "[300]\ttraining's auc: 0.728414\tvalid_1's auc: 0.673956\n",
      "[400]\ttraining's auc: 0.739566\tvalid_1's auc: 0.677509\n",
      "[500]\ttraining's auc: 0.74845\tvalid_1's auc: 0.678563\n",
      "[600]\ttraining's auc: 0.756144\tvalid_1's auc: 0.679651\n",
      "[700]\ttraining's auc: 0.763024\tvalid_1's auc: 0.680052\n",
      "[800]\ttraining's auc: 0.770257\tvalid_1's auc: 0.679954\n",
      "[900]\ttraining's auc: 0.776944\tvalid_1's auc: 0.680159\n",
      "[1000]\ttraining's auc: 0.782605\tvalid_1's auc: 0.679775\n",
      "Early stopping, best iteration is:\n",
      "[839]\ttraining's auc: 0.772769\tvalid_1's auc: 0.680251\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.68368\tvalid_1's auc: 0.657988\n",
      "[200]\ttraining's auc: 0.711173\tvalid_1's auc: 0.674617\n",
      "[300]\ttraining's auc: 0.726526\tvalid_1's auc: 0.681482\n",
      "[400]\ttraining's auc: 0.738045\tvalid_1's auc: 0.684142\n",
      "[500]\ttraining's auc: 0.748049\tvalid_1's auc: 0.686024\n",
      "[600]\ttraining's auc: 0.756866\tvalid_1's auc: 0.687112\n",
      "[700]\ttraining's auc: 0.763868\tvalid_1's auc: 0.687745\n",
      "[800]\ttraining's auc: 0.770742\tvalid_1's auc: 0.68747\n",
      "Early stopping, best iteration is:\n",
      "[687]\ttraining's auc: 0.763291\tvalid_1's auc: 0.687938\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.682702\tvalid_1's auc: 0.662084\n",
      "[200]\ttraining's auc: 0.710496\tvalid_1's auc: 0.67929\n",
      "[300]\ttraining's auc: 0.726217\tvalid_1's auc: 0.686493\n",
      "[400]\ttraining's auc: 0.73773\tvalid_1's auc: 0.689202\n",
      "[500]\ttraining's auc: 0.747169\tvalid_1's auc: 0.690169\n",
      "[600]\ttraining's auc: 0.755599\tvalid_1's auc: 0.690845\n",
      "[700]\ttraining's auc: 0.763386\tvalid_1's auc: 0.690901\n",
      "[800]\ttraining's auc: 0.769424\tvalid_1's auc: 0.691359\n",
      "[900]\ttraining's auc: 0.77526\tvalid_1's auc: 0.691117\n",
      "[1000]\ttraining's auc: 0.781694\tvalid_1's auc: 0.691429\n",
      "[1100]\ttraining's auc: 0.78653\tvalid_1's auc: 0.691318\n",
      "[1200]\ttraining's auc: 0.791519\tvalid_1's auc: 0.691067\n",
      "Early stopping, best iteration is:\n",
      "[1021]\ttraining's auc: 0.782705\tvalid_1's auc: 0.691535\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.684431\tvalid_1's auc: 0.659126\n",
      "[200]\ttraining's auc: 0.712838\tvalid_1's auc: 0.671552\n",
      "[300]\ttraining's auc: 0.728856\tvalid_1's auc: 0.676766\n",
      "[400]\ttraining's auc: 0.739308\tvalid_1's auc: 0.678903\n",
      "[500]\ttraining's auc: 0.74922\tvalid_1's auc: 0.679731\n",
      "[600]\ttraining's auc: 0.756755\tvalid_1's auc: 0.679719\n",
      "[700]\ttraining's auc: 0.763518\tvalid_1's auc: 0.67972\n",
      "[800]\ttraining's auc: 0.7697\tvalid_1's auc: 0.679527\n",
      "Early stopping, best iteration is:\n",
      "[662]\ttraining's auc: 0.760964\tvalid_1's auc: 0.679899\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.685369\tvalid_1's auc: 0.64918\n",
      "[200]\ttraining's auc: 0.712146\tvalid_1's auc: 0.664654\n",
      "[300]\ttraining's auc: 0.727546\tvalid_1's auc: 0.669758\n",
      "[400]\ttraining's auc: 0.738247\tvalid_1's auc: 0.671169\n",
      "[500]\ttraining's auc: 0.746993\tvalid_1's auc: 0.672428\n",
      "[600]\ttraining's auc: 0.754742\tvalid_1's auc: 0.673059\n",
      "[700]\ttraining's auc: 0.762061\tvalid_1's auc: 0.673505\n",
      "[800]\ttraining's auc: 0.767987\tvalid_1's auc: 0.673781\n",
      "[900]\ttraining's auc: 0.77423\tvalid_1's auc: 0.673891\n",
      "[1000]\ttraining's auc: 0.779844\tvalid_1's auc: 0.674035\n",
      "[1100]\ttraining's auc: 0.785606\tvalid_1's auc: 0.673377\n",
      "[1200]\ttraining's auc: 0.790594\tvalid_1's auc: 0.673172\n",
      "Early stopping, best iteration is:\n",
      "[1020]\ttraining's auc: 0.7809\tvalid_1's auc: 0.674139\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.684151\tvalid_1's auc: 0.657616\n",
      "[200]\ttraining's auc: 0.7114\tvalid_1's auc: 0.670995\n",
      "[300]\ttraining's auc: 0.728263\tvalid_1's auc: 0.677703\n",
      "[400]\ttraining's auc: 0.739108\tvalid_1's auc: 0.680065\n",
      "[500]\ttraining's auc: 0.749301\tvalid_1's auc: 0.682203\n",
      "[600]\ttraining's auc: 0.756704\tvalid_1's auc: 0.683031\n",
      "[700]\ttraining's auc: 0.763637\tvalid_1's auc: 0.683483\n",
      "[800]\ttraining's auc: 0.770027\tvalid_1's auc: 0.683722\n",
      "[900]\ttraining's auc: 0.776374\tvalid_1's auc: 0.68368\n",
      "[1000]\ttraining's auc: 0.782354\tvalid_1's auc: 0.684101\n",
      "[1100]\ttraining's auc: 0.787199\tvalid_1's auc: 0.684087\n",
      "[1200]\ttraining's auc: 0.792109\tvalid_1's auc: 0.68382\n",
      "Early stopping, best iteration is:\n",
      "[1066]\ttraining's auc: 0.785738\tvalid_1's auc: 0.684286\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.684525\tvalid_1's auc: 0.653027\n",
      "[200]\ttraining's auc: 0.711609\tvalid_1's auc: 0.66756\n",
      "[300]\ttraining's auc: 0.727127\tvalid_1's auc: 0.674881\n",
      "[400]\ttraining's auc: 0.738402\tvalid_1's auc: 0.677813\n",
      "[500]\ttraining's auc: 0.748276\tvalid_1's auc: 0.679236\n",
      "[600]\ttraining's auc: 0.756439\tvalid_1's auc: 0.679401\n",
      "[700]\ttraining's auc: 0.763768\tvalid_1's auc: 0.67906\n",
      "[800]\ttraining's auc: 0.770894\tvalid_1's auc: 0.678951\n",
      "Early stopping, best iteration is:\n",
      "[602]\ttraining's auc: 0.756524\tvalid_1's auc: 0.679457\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.684375\tvalid_1's auc: 0.65789\n",
      "[200]\ttraining's auc: 0.711898\tvalid_1's auc: 0.672778\n",
      "[300]\ttraining's auc: 0.728092\tvalid_1's auc: 0.678291\n",
      "[400]\ttraining's auc: 0.73871\tvalid_1's auc: 0.68139\n",
      "[500]\ttraining's auc: 0.748207\tvalid_1's auc: 0.682873\n",
      "[600]\ttraining's auc: 0.756702\tvalid_1's auc: 0.683311\n",
      "[700]\ttraining's auc: 0.763487\tvalid_1's auc: 0.682747\n",
      "Early stopping, best iteration is:\n",
      "[593]\ttraining's auc: 0.75614\tvalid_1's auc: 0.683332\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.68274\tvalid_1's auc: 0.658573\n",
      "[200]\ttraining's auc: 0.711019\tvalid_1's auc: 0.674947\n",
      "[300]\ttraining's auc: 0.72816\tvalid_1's auc: 0.682356\n",
      "[400]\ttraining's auc: 0.738782\tvalid_1's auc: 0.684572\n",
      "[500]\ttraining's auc: 0.748098\tvalid_1's auc: 0.685535\n",
      "[600]\ttraining's auc: 0.756668\tvalid_1's auc: 0.685974\n",
      "[700]\ttraining's auc: 0.76388\tvalid_1's auc: 0.685973\n",
      "[800]\ttraining's auc: 0.770844\tvalid_1's auc: 0.686194\n",
      "[900]\ttraining's auc: 0.776735\tvalid_1's auc: 0.686129\n",
      "[1000]\ttraining's auc: 0.782724\tvalid_1's auc: 0.686273\n",
      "[1100]\ttraining's auc: 0.787512\tvalid_1's auc: 0.686132\n",
      "[1200]\ttraining's auc: 0.791895\tvalid_1's auc: 0.685856\n",
      "Early stopping, best iteration is:\n",
      "[1015]\ttraining's auc: 0.783334\tvalid_1's auc: 0.686391\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.683627\tvalid_1's auc: 0.653987\n",
      "[200]\ttraining's auc: 0.711906\tvalid_1's auc: 0.668283\n",
      "[300]\ttraining's auc: 0.727343\tvalid_1's auc: 0.673565\n",
      "[400]\ttraining's auc: 0.737569\tvalid_1's auc: 0.674773\n",
      "[500]\ttraining's auc: 0.747037\tvalid_1's auc: 0.67556\n",
      "[600]\ttraining's auc: 0.755638\tvalid_1's auc: 0.675737\n",
      "[700]\ttraining's auc: 0.762932\tvalid_1's auc: 0.675458\n",
      "Early stopping, best iteration is:\n",
      "[589]\ttraining's auc: 0.754804\tvalid_1's auc: 0.675799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakhtar0092/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/sakhtar0092/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "meta_train = np.zeros(train_b.shape[0])\n",
    "meta_test = []\n",
    "feature_importance = pd.DataFrame(columns = [\"feature\",\"split\",\"gain\"])\n",
    "\n",
    "kf = StratifiedKFold(n_splits= 10, shuffle=True, random_state=47)\n",
    "for fold_id, (train_index, test_index) in enumerate(kf.split(train_b, target)):\n",
    "    x_train, x_test = train_b.iloc[train_index], train_b.iloc[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "\n",
    "    model_tree(x_train, x_test, y_train, y_test, test_b, meta_train, meta_test,train_index, test_index,fold_id)\n",
    "\n",
    "test_b_index[\"TARGET\"] = np.array(meta_test).T.mean(axis=1)\n",
    "train_b_index[\"TARGET\"] = meta_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE: 307511, TEST SHAPE: 48744\n"
     ]
    }
   ],
   "source": [
    "test = test[[\"SK_ID_CURR\"]].merge(test_b_index, how = \"left\", on = \"SK_ID_CURR\").fillna(0.5)\n",
    "train = train[[\"SK_ID_CURR\"]].merge(train_b_index, how = \"left\", on = \"SK_ID_CURR\").fillna(0.5)\n",
    "\n",
    "print \"TRAIN SHAPE: {}, TEST SHAPE: {}\".format(train.shape[0],test.shape[0])\n",
    "\n",
    "train.to_csv(\"csv/{}_train.csv\".format(model_name), index=False)\n",
    "test.to_csv(\"csv/{}_test.csv\".format(model_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Feature Importance\n",
    "feature_importance = feature_importance.groupby(\"feature\")[\"gain\"].mean().reset_index().sort_values('gain', ascending=False).reset_index(drop=True)\n",
    "plt.figure()\n",
    "feature_importance[['feature','gain']].head(60).plot(kind='barh', x='feature', y='gain', legend=False, figsize=(30, 100))\n",
    "plt.gcf().savefig('csv/{}.png'.format(model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
